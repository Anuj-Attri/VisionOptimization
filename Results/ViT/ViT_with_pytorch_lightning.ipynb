{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "PBaUQFBjit-c"
      },
      "outputs": [],
      "source": [
        "# !pip install -q torchinfo\n",
        "# !pip install -q pytorch_lightning\n",
        "# !pip install -q timm\n",
        "# !pip install -q einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import torchvision.transforms as T\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "from time import time\n",
        "from datetime import timedelta\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim import Adam\n",
        "from torch.cuda.amp import GradScaler\n",
        "from timm.scheduler import CosineLRScheduler\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "eNC9ir0Rk5aK"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Data\n",
        "VAL_RATIO = 0.1\n",
        "### CIFAR-10\n",
        "DATA_DIR = \"/content/data/cifar-10-batches-py\"\n",
        "with open(Path(DATA_DIR)/\"batches.meta\", mode=\"rb\") as f:\n",
        "    meta = pickle.load(f, encoding=\"bytes\")\n",
        "label_names = meta[b\"label_names\"]\n",
        "CIFAR10_CLASSES = [i.decode(\"ascii\") for i in label_names]\n",
        "N_CLASSES = len(CIFAR10_CLASSES)\n",
        "IMG_SIZE = 32\n",
        "\n",
        "### Architecture\n",
        "DROP_PROB = 0.1\n",
        "N_LAYERS = 6\n",
        "HIDDEN_SIZE = 384\n",
        "MLP_SIZE = 384\n",
        "N_HEADS = 12\n",
        "PATCH_SIZE = 4\n",
        "\n",
        "### Optimizer\n",
        "# \"Adam with $beta_{1} = 0.9$, $beta_{2}= 0.999$, a batch size of 4096 and apply a high weight decay\n",
        "# of 0.1, which we found to be useful for transfer of all models.\"\n",
        "BASE_LR = 1e-3\n",
        "BETA1 = 0.9\n",
        "BETA2 = 0.999\n",
        "WEIGHT_DECAY = 5e-5\n",
        "WARMUP_EPOCHS = 5\n",
        "\n",
        "### Regularization\n",
        "SMOOTHING = 0.1 # If `0`, do not employ label smoothing\n",
        "CUTMIX = False\n",
        "CUTOUT = False\n",
        "HIDE_AND_SEEK = False\n",
        "\n",
        "### Training\n",
        "SEED = 17\n",
        "# BATCH_SIZE = 4096 # \"All models are trained with a batch size of 4096.\"\n",
        "BATCH_SIZE = 2048\n",
        "N_EPOCHS = 300\n",
        "N_WORKERS = 6\n",
        "N_GPUS = torch.cuda.device_count()\n",
        "if N_GPUS > 0:\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    print(f\"\"\"Using {N_GPUS} GPU(s).\"\"\")\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    print(f\"\"\"Using CPU(s).\"\"\")\n",
        "MULTI_GPU = True\n",
        "AUTOCAST = True\n",
        "N_PRINT_EPOCHS = 4\n",
        "N_VAL_EPOCHS = 4\n",
        "CKPT_DIR = Path(\"/content/\").parent/\"checkpoints\"\n",
        "\n",
        "### Resume\n",
        "CKPT_PATH = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZXZmufypkN-",
        "outputId": "c032c631-78b7-4a82-ef6c-bc58a5accb6b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 1 GPU(s).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, patch_size, hidden_size, drop_prob=DROP_PROB):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        dim = (patch_size ** 2) * 3\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.proj = nn.Linear(dim, hidden_size)\n",
        "        self.drop = nn.Dropout(drop_prob)\n",
        "        self.norm2 = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = rearrange(\n",
        "            x,\n",
        "            pattern=\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\",\n",
        "            p1=self.patch_size,\n",
        "            p2=self.patch_size,\n",
        "        )\n",
        "        x = self.norm1(x) # Not in the paper\n",
        "        x = self.proj(x)\n",
        "        # \"Dropout is applied after every dense layer except for the the qkv-projections\n",
        "        # and directly after adding positional- to patch embeddings.\"\n",
        "        x = self.drop(x)\n",
        "        x = self.norm2(x) # Not in the paper\n",
        "        return x\n",
        "\n",
        "\n",
        "class MSA(nn.Module):\n",
        "    def __init__(self, hidden_size, n_heads, drop_prob=DROP_PROB):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head_size = hidden_size // n_heads\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        # \"U_{qkv} \\in \\mathbb{R}^{D \\times 3D_{h}}\"\n",
        "        self.qkv_proj = nn.Linear(hidden_size, 3 * n_heads * self.head_size, bias=False)\n",
        "        self.drop = nn.Dropout(drop_prob)\n",
        "        self.out_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "    def _get_attention_score(self, q, k):\n",
        "        # \"$qk^{T}$\"\n",
        "        attn_score = torch.einsum(\"bhnd,bhmd->bhnm\", q, k)\n",
        "        return attn_score\n",
        "\n",
        "    def forward(self, x):\n",
        "        # \"$[q, k, v] = zU_{qkv}$\"\n",
        "        q, k, v = torch.split(\n",
        "            self.qkv_proj(x), split_size_or_sections=self.n_heads * self.head_size, dim=2,\n",
        "        )\n",
        "        q = rearrange(q, pattern=\"b n (h d) -> b h n d\", h=self.n_heads, d=self.head_size)\n",
        "        k = rearrange(k, pattern=\"b n (h d) -> b h n d\", h=self.n_heads, d=self.head_size)\n",
        "        v = rearrange(v, pattern=\"b n (h d) -> b h n d\", h=self.n_heads, d=self.head_size)\n",
        "        attn_score = self._get_attention_score(q=q, k=k)\n",
        "        # \"$A = softmax(qk^{T}/\\sqrt{D_{h}}), A \\in \\mathbb{R}^{N \\times N}$\"\n",
        "        attn_weight = F.softmax(attn_score / (self.head_size ** 0.5), dim=3)\n",
        "        # attn_weight = self.drop(attn_weight)\n",
        "        x = torch.einsum(\"bhnm,bhmd->bhnd\", attn_weight, v)\n",
        "        # \"$U_{msa} \\in \\mathbb{R}^{k \\cdot D_{h} \\times D}$\"\n",
        "        x = rearrange(x, pattern=\"b h n d -> b n (h d)\")\n",
        "        x = self.out_proj(x)\n",
        "        # \"Dropout is applied after every dense layer except for the the qkv-projections\n",
        "        # and directly after adding positional- to patch embeddings.\"\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SkipConnection(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm = nn.LayerNorm(hidden_size) # \"$LN$\"\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        # \"Layernorm (LN) is applied before every block, and residual connections after every block.\"\n",
        "        # \"$z'_{l} = MSA(LN(z_{l - 1})) + z_{l - 1}$\", \"$z_{l} = MLP(LN(z'_{l})) + z'_{l}$\"\n",
        "        skip = x.clone()\n",
        "        x = self.norm(x)\n",
        "        x = sublayer(x)\n",
        "        x += skip\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden_size, mlp_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.proj1 = nn.Linear(hidden_size, mlp_size)\n",
        "        self.drop1 = nn.Dropout(0.1)\n",
        "        self.proj2 = nn.Linear(mlp_size, hidden_size)\n",
        "        self.drop2 = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj1(x)\n",
        "        x = F.gelu(x) # \"The MLP contains two layers with a GELU non-linearity.\"\n",
        "        # \"Dropout is applied after every dense layer except for the the qkv-projections\n",
        "        # and directly after adding positional- to patch embeddings.\"\n",
        "        # Activation function 다음에 Dropout이 오도록!\n",
        "        x = self.drop1(x)\n",
        "        x = self.proj2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.drop2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, mlp_size, n_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn = MSA(hidden_size=hidden_size, n_heads=n_heads)\n",
        "        self.self_attn_resid = SkipConnection(hidden_size=hidden_size)\n",
        "        self.mlp = MLP(hidden_size=hidden_size, mlp_size=mlp_size)\n",
        "        self.mlp_resid = SkipConnection(hidden_size=hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.self_attn_resid(x=x, sublayer=self.self_attn)\n",
        "        x = self.mlp_resid(x=x, sublayer=self.mlp)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, n_layers, hidden_size, mlp_size, n_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc_stack = nn.ModuleList(\n",
        "            [TransformerEncoderLayer(hidden_size=hidden_size, mlp_size=mlp_size, n_heads=n_heads)\n",
        "                for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for enc_layer in self.enc_stack:\n",
        "            x = enc_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"\n",
        "    ViT-Base: `n_layers=12, hidden_size=768, mlp_size=3072, n_heads=12`\n",
        "    ViT-Large: `n_layers=24, hidden_size=1024, mlp_size=4096, n_heads=16`\n",
        "    ViT-Huge: `n_layers=32, hidden_size=1280, mlp_size=5120, n_heads=16`\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        n_layers=12,\n",
        "        hidden_size=768,\n",
        "        mlp_size=3072,\n",
        "        n_heads=12,\n",
        "        drop_prob=DROP_PROB,\n",
        "        n_classes=0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        assert img_size % patch_size == 0, \"`img_size` must be divisible by `patch_size`!\"\n",
        "\n",
        "        cell_size = img_size // patch_size\n",
        "        n_patches = cell_size ** 2\n",
        "\n",
        "        # $\\textbf{E}$ of the equation 1 in the paper\n",
        "        self.patch_embed = PatchEmbedding(patch_size=patch_size, hidden_size=hidden_size)\n",
        "        self.cls_token = nn.Parameter(torch.randn((1, 1, hidden_size))) # $x_{\\text{class}}$\n",
        "        # $\\textbf{E}_\\text{pos}$\n",
        "        self.pos_embed = nn.Parameter(torch.randn((1, n_patches + 1, hidden_size)))\n",
        "        self.drop1 = nn.Dropout(drop_prob)\n",
        "        self.tf_enc = TransformerEncoder(\n",
        "            n_layers=n_layers, hidden_size=hidden_size, mlp_size=mlp_size, n_heads=n_heads,\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(hidden_size) # \"$LN$\"\n",
        "        self.proj = nn.Linear(hidden_size, n_classes)\n",
        "        self.drop2 = nn.Dropout(drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, _, _, _ = x.shape\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "        x = torch.cat((self.cls_token.repeat(b, 1, 1), x), dim=1)\n",
        "        x += self.pos_embed\n",
        "        # \"Dropout is applied after every dense layer except for the the qkv-projections\n",
        "        # and directly after adding positional- to patch embeddings.\"\n",
        "        x = self.drop1(x)\n",
        "        x = self.tf_enc(x)\n",
        "\n",
        "        if self.n_classes == 0:\n",
        "            x = x.mean(dim=1)\n",
        "        else:\n",
        "            x = x[:, 0, :] # $z^{0}_{L}$ of the equation 4 in the paper\n",
        "            # \"Layernorm (LN) is applied before every block.\"\n",
        "            x = self.norm(x) # $y$\n",
        "            x = self.proj(x)\n",
        "            # \"Dropout is applied after every dense layer except for the the qkv-projections\n",
        "            # and directly after adding positional- to patch embeddings.\"\n",
        "            x = self.drop2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ALTMEU0-iyO8"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_parameters(model):\n",
        "    print(f\"\"\"{sum([p.numel() for p in model.parameters()]):,}\"\"\")\n",
        "\n",
        "\n",
        "def get_elapsed_time(start_time):\n",
        "    return timedelta(seconds=round(time() - start_time))\n",
        "\n",
        "\n",
        "def load_image(img_path):\n",
        "    img_path = str(img_path)\n",
        "    img = cv2.imread(img_path, flags=cv2.IMREAD_COLOR)\n",
        "    img = cv2.cvtColor(src=img, code=cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "\n",
        "def _to_pil(img):\n",
        "    if not isinstance(img, Image.Image):\n",
        "        img = Image.fromarray(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def show_image(img):\n",
        "    copied = img.copy()\n",
        "    copied = _to_pil(copied)\n",
        "    copied.show()\n",
        "\n",
        "\n",
        "def _apply_jet_colormap(img):\n",
        "    img_jet = cv2.applyColorMap(src=(255 - img), colormap=cv2.COLORMAP_JET)\n",
        "    return img_jet\n",
        "\n",
        "\n",
        "def _to_array(img):\n",
        "    img = np.array(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def _blend_two_images(img1, img2, alpha=0.5):\n",
        "    img1 = _to_pil(img1)\n",
        "    img2 = _to_pil(img2)\n",
        "    img_blended = Image.blend(im1=img1, im2=img2, alpha=alpha)\n",
        "    return _to_array(img_blended)\n",
        "\n",
        "\n",
        "def _to_3d(img):\n",
        "    if img.ndim == 2:\n",
        "        return np.dstack([img, img, img])\n",
        "    else:\n",
        "        return img\n",
        "\n",
        "\n",
        "def _rgba_to_rgb(img):\n",
        "    copied = img.copy().astype(\"float\")\n",
        "    copied[..., 0] *= copied[..., 3] / 255\n",
        "    copied[..., 1] *= copied[..., 3] / 255\n",
        "    copied[..., 2] *= copied[..., 3] / 255\n",
        "    copied = copied.astype(\"uint8\")\n",
        "    copied = copied[..., : 3]\n",
        "    return copied\n",
        "\n",
        "\n",
        "def _preprocess_image(img):\n",
        "    if img.dtype == \"bool\":\n",
        "        img = img.astype(\"uint8\") * 255\n",
        "\n",
        "    if img.ndim == 2:\n",
        "        if (\n",
        "            np.array_equal(np.unique(img), np.array([0, 255])) or\n",
        "            np.array_equal(np.unique(img), np.array([0])) or\n",
        "            np.array_equal(np.unique(img), np.array([255]))\n",
        "        ):\n",
        "            img = _to_3d(img)\n",
        "        else:\n",
        "            img = _apply_jet_colormap(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def _blend_two_images(img1, img2, alpha=0.5):\n",
        "    img1 = _to_pil(img1)\n",
        "    img2 = _to_pil(img2)\n",
        "    img_blended = Image.blend(im1=img1, im2=img2, alpha=alpha)\n",
        "    return _to_array(img_blended)\n",
        "\n",
        "\n",
        "def save_image(img1, img2=None, alpha=0.5, path=\"\") -> None:\n",
        "    copied1 = _preprocess_image(\n",
        "        _to_array(img1.copy())\n",
        "    )\n",
        "    if img2 is None:\n",
        "        img_arr = copied1\n",
        "    else:\n",
        "        copied2 = _to_array(\n",
        "            _preprocess_image(\n",
        "                _to_array(img2.copy())\n",
        "            )\n",
        "        )\n",
        "        img_arr = _to_array(\n",
        "            _blend_two_images(img1=copied1, img2=copied2, alpha=alpha)\n",
        "        )\n",
        "\n",
        "    path = Path(path)\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if img_arr.ndim == 3:\n",
        "        cv2.imwrite(\n",
        "            filename=str(path), img=img_arr[:, :, :: -1], params=[cv2.IMWRITE_JPEG_QUALITY, 100]\n",
        "        )\n",
        "    elif img_arr.ndim == 2:\n",
        "        cv2.imwrite(\n",
        "            filename=str(path), img=img_arr, params=[cv2.IMWRITE_JPEG_QUALITY, 100]\n",
        "        )\n",
        "\n",
        "\n",
        "def denorm(tensor, mean, std):\n",
        "    return TF.normalize(\n",
        "        tensor, mean=- np.array(mean) / np.array(std), std=1 / np.array(std),\n",
        "    )\n",
        "\n",
        "\n",
        "def image_to_grid(image, mean, std, n_cols, padding=1):\n",
        "    tensor = image.clone().detach().cpu()\n",
        "    tensor = denorm(tensor, mean=mean, std=std)\n",
        "    grid = make_grid(tensor, nrow=n_cols, padding=1, pad_value=padding)\n",
        "    grid.clamp_(0, 1)\n",
        "    grid = TF.to_pil_image(grid)\n",
        "    return grid"
      ],
      "metadata": {
        "id": "DN6BFQibqlAG"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loaders\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJTu-S9coFZ5",
        "outputId": "b5dfb997-3907-4dc5-d368-da2a264e6451"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CELossWithLabelSmoothing(nn.Module):\n",
        "    def __init__(self, n_classes, smoothing=0):\n",
        "        super().__init__()\n",
        "\n",
        "        assert 0 <= smoothing <= 1, \"The argument `smoothing` must be between 0 and 1!\"\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, pred, gt):\n",
        "        if gt.ndim == 1:\n",
        "            gt = torch.eye(self.n_classes, device=gt.device)[gt]\n",
        "            return self(pred, gt)\n",
        "        elif gt.ndim == 2:\n",
        "            log_prob = F.log_softmax(pred, dim=1)\n",
        "            ce_loss = -torch.sum(gt * log_prob, dim=1)\n",
        "            loss = (1 - self.smoothing) * ce_loss\n",
        "            loss += self.smoothing * -torch.sum(log_prob, dim=1)\n",
        "            return torch.mean(loss)\n",
        "\n",
        "\n",
        "class ClassificationLoss(nn.Module):\n",
        "    def __init__(self, n_classes, smoothing=0):\n",
        "        super().__init__()\n",
        "\n",
        "        assert 0 <= smoothing <= 1, \"The argument `smoothing` must be between 0 and 1!\"\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "        self.ce = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "    def forward(self, pred, gt):\n",
        "        if gt.ndim == 1:\n",
        "            new_gt = torch.full_like(pred, fill_value=self.smoothing / (self.n_classes - 1))\n",
        "            new_gt.scatter_(1, gt.unsqueeze(1), 1 - self.smoothing)\n",
        "        elif gt.ndim == 2:\n",
        "            new_gt = gt.clone()\n",
        "            new_gt.sum(dim=1)\n",
        "            new_gt *= (1 - self.smoothing)\n",
        "            is_zero = (gt == 0)\n",
        "            likelihood = self.smoothing / (gt.shape[1] - (~is_zero).sum(dim=1))\n",
        "            new_gt += is_zero * likelihood.unsqueeze(1).repeat(1, self.n_classes)\n",
        "        loss = self.ce(pred, new_gt)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "TA1m4PRfqrHz"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TopKAccuracy(nn.Module):\n",
        "    def __init__(self, k):\n",
        "        super().__init__()\n",
        "\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, pred, gt):\n",
        "        _, topk = torch.topk(pred, k=self.k, dim=1)\n",
        "        corr = torch.eq(topk, gt.unsqueeze(1).repeat(1, self.k))\n",
        "        acc = corr.sum(dim=1).float().mean().item()\n",
        "        return acc"
      ],
      "metadata": {
        "id": "_0CkURM4q16o"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cifar10_imgs_and_gts(data_path):\n",
        "    with open(data_path, mode=\"rb\") as f:\n",
        "        data_dic = pickle.load(f, encoding=\"bytes\")\n",
        "\n",
        "    imgs = data_dic[b\"data\"]\n",
        "    imgs = imgs.reshape(-1, 3, IMG_SIZE, IMG_SIZE)\n",
        "    imgs = imgs.transpose(0, 2, 3, 1)\n",
        "\n",
        "    gts = data_dic[b\"labels\"]\n",
        "    gts = np.array(gts)\n",
        "    return imgs, gts\n",
        "\n",
        "\n",
        "def get_cifar10_train_val_set(data_dir):\n",
        "    imgs_ls = list()\n",
        "    gts_ls = list()\n",
        "    for idx in range(1, 6):\n",
        "        imgs, gts = get_cifar10_imgs_and_gts(Path(data_dir)/f\"data_batch_{idx}\")\n",
        "        imgs_ls.append(imgs)\n",
        "        gts_ls.append(gts)\n",
        "    imgs = np.concatenate(imgs_ls, axis=0)\n",
        "    gts = np.concatenate(gts_ls, axis=0)\n",
        "    return imgs, gts\n",
        "\n",
        "\n",
        "def get_all_cifar10_imgs_and_gts(data_dir, val_ratio):\n",
        "    train_val_imgs, train_val_gts = get_cifar10_train_val_set(data_dir)\n",
        "    train_imgs, val_imgs, train_gts, val_gts = train_test_split(\n",
        "        train_val_imgs, train_val_gts, test_size=val_ratio,\n",
        "    )\n",
        "    test_imgs, test_gts = get_cifar10_imgs_and_gts(Path(data_dir)/\"test_batch\")\n",
        "    return train_imgs, train_gts, val_imgs, val_gts, test_imgs, test_gts\n",
        "\n",
        "\n",
        "def get_cifar_mean_and_std(imgs):\n",
        "    imgs = imgs.astype(\"float\") / 255\n",
        "    n_pixels = imgs.size // 3\n",
        "    sum_ = imgs.reshape(-1, 3).sum(axis=0)\n",
        "    sum_square = (imgs ** 2).reshape(-1, 3).sum(axis=0)\n",
        "    mean = (sum_ / n_pixels).round(3)\n",
        "    std = (((sum_square / n_pixels) - mean ** 2) ** 0.5).round(3)\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "class CIFARDataset(Dataset):\n",
        "    def __init__(self, imgs, gts, mean, std):\n",
        "        super().__init__()\n",
        "\n",
        "        self.imgs = imgs\n",
        "        self.gts = gts\n",
        "\n",
        "        self.transform = T.Compose([\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "            T.RandomCrop(size=IMG_SIZE, padding=4, pad_if_needed=True),\n",
        "            T.RandomApply(\n",
        "                [T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)],\n",
        "                p=0.4,\n",
        "            ),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=mean, std=std),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.gts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.imgs[idx]\n",
        "        image = Image.fromarray(img, mode=\"RGB\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        gt = self.gts[idx]\n",
        "        gt = torch.tensor(gt).long()\n",
        "        return image, gt\n",
        "\n",
        "\n",
        "def get_cifar10_dses(data_dir, val_ratio=0.1):\n",
        "    train_imgs, train_gts, val_imgs, val_gts, test_imgs, test_gts = get_all_cifar10_imgs_and_gts(\n",
        "            data_dir=data_dir, val_ratio=val_ratio,\n",
        "    )\n",
        "    mean, std = get_cifar_mean_and_std(train_imgs)\n",
        "    train_ds = CIFARDataset(imgs=train_imgs, gts=train_gts, mean=mean, std=std)\n",
        "    val_ds = CIFARDataset(imgs=val_imgs, gts=val_gts, mean=mean, std=std)\n",
        "    test_ds = CIFARDataset(imgs=test_imgs, gts=test_gts, mean=mean, std=std)\n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "EVqaD76jrAan"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def apply_cutmix(image, gt, n_classes):\n",
        "    if gt.ndim == 1:\n",
        "        gt = F.one_hot(gt, num_classes=n_classes)\n",
        "\n",
        "    b, _, h, w = image.shape\n",
        "\n",
        "    lamb = random.random()\n",
        "    region_x = random.randint(0, w)\n",
        "    region_y = random.randint(0, h)\n",
        "    region_w = region_h = (1 - lamb) ** 0.5\n",
        "\n",
        "    xmin = max(0, int(region_x - region_w / 2))\n",
        "    ymin = max(0, int(region_y - region_h / 2))\n",
        "    xmax = max(w, int(region_x + region_w / 2))\n",
        "    ymax = max(h, int(region_y + region_h / 2))\n",
        "\n",
        "    indices = torch.randperm(b)\n",
        "    image[:, :, ymin: ymax, xmin: xmax] = image[indices][:, :, ymin: ymax, xmin: xmax]\n",
        "    lamb = 1 - (xmax - xmin) * (ymax - ymin) / (w * h)\n",
        "    gt = lamb * gt + (1 - lamb) * gt[indices]\n",
        "    return image, gt\n",
        "\n",
        "\n",
        "\n",
        "def apply_hide_and_seek(image, patch_size, hide_prob=0.5, mean=(0.5, 0.5, 0.5)):\n",
        "    b, _, h, w = image.shape\n",
        "    assert h % patch_size == 0 and w % patch_size == 0,\\\n",
        "        \"`patch_size` argument should be a multiple of both the width and height of the input image\"\n",
        "\n",
        "    mean_tensor = torch.Tensor(mean)[None, :, None, None].repeat(b, 1, patch_size, patch_size)\n",
        "\n",
        "    copied = image.clone()\n",
        "    for i in range(h // patch_size):\n",
        "        for j in range(w // patch_size):\n",
        "            if random.random() < hide_prob:\n",
        "                    continue\n",
        "\n",
        "            copied[\n",
        "                ..., i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size\n",
        "            ] = mean_tensor\n",
        "    return copied\n",
        "\n",
        "def apply_cutout(image, cutout_size=16, mean=(0.485, 0.456, 0.406)):\n",
        "  _, _, h, w = image.shape\n",
        "\n",
        "  x = random.randint(0, w)\n",
        "  y = random.randint(0, h)\n",
        "  xmin = max(0, x - cutout_size // 2)\n",
        "  ymin = max(0, y - cutout_size // 2)\n",
        "  xmax = max(0, x + cutout_size // 2)\n",
        "  ymax = max(0, y + cutout_size // 2)\n",
        "\n",
        "  image[:, 0, ymin: ymax, xmin: xmax] = mean[0]\n",
        "  image[:, 1, ymin: ymax, xmin: xmax] = mean[1]\n",
        "  image[:, 2, ymin: ymax, xmin: xmax] = mean[2]\n",
        "  return image"
      ],
      "metadata": {
        "id": "FGfRdi_wrcFR"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorBoard Logger\n",
        "logger = TensorBoardLogger('tb_logs', name='vit_experiment')"
      ],
      "metadata": {
        "id": "9kZz8x4kkIkX"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(linewidth=200, sci_mode=False)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "\n",
        "def save_checkpoint(epoch, model, optim, scaler, avg_acc, ckpt_path):\n",
        "    Path(ckpt_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ckpt = {\n",
        "        \"epoch\": epoch,\n",
        "        \"optimizer\": optim.state_dict(),\n",
        "        \"scaler\": scaler.state_dict(),\n",
        "        \"average_accuracy\": avg_acc,\n",
        "    }\n",
        "    if N_GPUS > 0 and MULTI_GPU:\n",
        "        ckpt[\"model\"] = model.module.state_dict()\n",
        "    else:\n",
        "        ckpt[\"model\"] = model.state_dict()\n",
        "\n",
        "    torch.save(ckpt, str(ckpt_path))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(dl, model, metric):\n",
        "    print(f\"\"\"Validating...\"\"\")\n",
        "    model.eval()\n",
        "    sum_acc = 0\n",
        "    for image, gt in dl:\n",
        "        image = image.to(DEVICE)\n",
        "        gt = gt.to(DEVICE)\n",
        "\n",
        "        pred = model(image)\n",
        "        acc = metric(pred=pred, gt=gt)\n",
        "        sum_acc += acc\n",
        "    avg_acc = sum_acc / len(dl)\n",
        "    print(f\"\"\"Average accuracy: {avg_acc:.3f}\"\"\")\n",
        "\n",
        "    model.train()\n",
        "    return avg_acc\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"\"\"N_WORKERS = {N_WORKERS}\"\"\")\n",
        "    print(f\"\"\"DEVICE = {DEVICE}\"\"\")\n",
        "    print(f\"\"\"AUTOCAST = {AUTOCAST}\"\"\")\n",
        "    print(f\"\"\"BATCH_SIZE = {BATCH_SIZE}\"\"\")\n",
        "\n",
        "    train_ds, val_ds, test_ds = get_cifar10_dses(data_dir=DATA_DIR, val_ratio=VAL_RATIO)\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, drop_last=True,\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, drop_last=True,\n",
        "    )\n",
        "\n",
        "    model = ViT(\n",
        "        img_size=IMG_SIZE,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        n_layers=N_LAYERS,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        mlp_size=MLP_SIZE,\n",
        "        n_heads=N_HEADS,\n",
        "        n_classes=N_CLASSES,\n",
        "    )\n",
        "    if N_GPUS > 0:\n",
        "        model = model.to(DEVICE)\n",
        "        if MULTI_GPU:\n",
        "            model = nn.DataParallel(model)\n",
        "\n",
        "    crit = CELossWithLabelSmoothing(n_classes=N_CLASSES, smoothing=SMOOTHING)\n",
        "    metric = TopKAccuracy(k=1)\n",
        "\n",
        "    optim = Adam(\n",
        "        model.parameters(),\n",
        "        lr=BASE_LR,\n",
        "        betas=(BETA1, BETA2),\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "    )\n",
        "    scheduler = CosineLRScheduler(\n",
        "        optimizer=optim,\n",
        "        t_initial=N_EPOCHS,\n",
        "        warmup_t=WARMUP_EPOCHS,\n",
        "        warmup_lr_init=BASE_LR / 10,\n",
        "        t_in_epochs=True,\n",
        "    )\n",
        "\n",
        "    scaler = GradScaler(enabled=True if AUTOCAST else False)\n",
        "\n",
        "    ### Resume\n",
        "    if CKPT_PATH is not None:\n",
        "        ckpt = torch.load(CKPT_PATH, map_location=DEVICE)\n",
        "        if N_GPUS > 1 and MULTI_GPU:\n",
        "            model.module.load_state_dict(ckpt[\"model\"])\n",
        "        else:\n",
        "            model.load_state_dict(ckpt[\"model\"])\n",
        "        optim.load_state_dict(ckpt[\"optimizer\"])\n",
        "        scaler.load_state_dict(ckpt[\"scaler\"])\n",
        "\n",
        "        init_epoch = ckpt[\"epoch\"]\n",
        "        best_avg_acc = ckpt[\"average_accuracy\"]\n",
        "        print(f\"\"\"Resuming from checkpoint '{CKPT_PATH}'...\"\"\")\n",
        "\n",
        "        prev_ckpt_path = CKPT_PATH\n",
        "    else:\n",
        "        init_epoch = 0\n",
        "        prev_ckpt_path = \".pth\"\n",
        "        best_avg_acc = 0\n",
        "\n",
        "    start_time = time()\n",
        "    running_loss = 0\n",
        "    step_cnt = 0\n",
        "    for epoch in range(init_epoch + 1, N_EPOCHS + 1):\n",
        "        for step, (image, gt) in enumerate(train_dl, start=1):\n",
        "            image = image.to(DEVICE)\n",
        "            gt = gt.to(DEVICE)\n",
        "\n",
        "            if HIDE_AND_SEEK:\n",
        "                image = apply_hide_and_seek(\n",
        "                    image, patch_size=IMG_SIZE // 4, mean=MEAN,\n",
        "                )\n",
        "            if CUTMIX:\n",
        "                image, gt = apply_cutmix(image=image, gt=gt, n_classes=N_CLASSES)\n",
        "            if CUTOUT:\n",
        "                image = apply_cutout(image)\n",
        "\n",
        "            with torch.autocast(\n",
        "                device_type=DEVICE.type,\n",
        "                dtype=torch.float16,\n",
        "                enabled=True if AUTOCAST else False,\n",
        "            ):\n",
        "                pred = model(image)\n",
        "                loss = crit(pred, gt)\n",
        "            optim.zero_grad()\n",
        "            if AUTOCAST:\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optim)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "            scheduler.step_update(num_updates=epoch * len(train_dl))\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            step_cnt += 1\n",
        "\n",
        "        if (epoch % N_PRINT_EPOCHS == 0) or (epoch == N_EPOCHS):\n",
        "            loss = running_loss / step_cnt\n",
        "            lr = optim.param_groups[0]['lr']\n",
        "            print(f\"\"\"[ {epoch:,}/{N_EPOCHS} ][ {step:,}/{len(train_dl):,} ]\"\"\", end=\"\")\n",
        "            print(f\"\"\"[ {lr:.5f} ][ {get_elapsed_time(start_time)} ][ {loss:.2f} ]\"\"\")\n",
        "\n",
        "            running_loss = 0\n",
        "            step_cnt = 0\n",
        "            start_time = time()\n",
        "\n",
        "        if (epoch % N_VAL_EPOCHS == 0) or (epoch == N_EPOCHS):\n",
        "            avg_acc = validate(dl=val_dl, model=model, metric=metric)\n",
        "            if avg_acc > best_avg_acc:\n",
        "                cur_ckpt_path = CKPT_DIR/f\"\"\"epoch_{epoch}_avg_acc_{round(avg_acc, 3)}.pth\"\"\"\n",
        "                save_checkpoint(\n",
        "                    epoch=epoch,\n",
        "                    model=model,\n",
        "                    optim=optim,\n",
        "                    scaler=scaler,\n",
        "                    avg_acc=avg_acc,\n",
        "                    ckpt_path=cur_ckpt_path,\n",
        "                )\n",
        "                print(f\"\"\"Saved checkpoint.\"\"\")\n",
        "                prev_ckpt_path = Path(prev_ckpt_path)\n",
        "                if prev_ckpt_path.exists():\n",
        "                    prev_ckpt_path.unlink()\n",
        "\n",
        "                best_avg_acc = avg_acc\n",
        "                prev_ckpt_path = cur_ckpt_path\n",
        "\n",
        "        scheduler.step(epoch + 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SUYZVVPMk1H_",
        "outputId": "f711f136-94a4-46b9-83ed-6e12e4f533bc"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N_WORKERS = 6\n",
            "DEVICE = cuda\n",
            "AUTOCAST = True\n",
            "BATCH_SIZE = 2048\n",
            "[ 4/300 ][ 21/21 ][ 0.00082 ][ 0:03:35 ][ 4.23 ]\n",
            "Validating...\n",
            "Average accuracy: 0.456\n",
            "Saved checkpoint.\n",
            "[ 8/300 ][ 21/21 ][ 0.00100 ][ 0:03:36 ][ 4.05 ]\n",
            "Validating...\n",
            "Average accuracy: 0.551\n",
            "Saved checkpoint.\n",
            "[ 12/300 ][ 21/21 ][ 0.00100 ][ 0:03:37 ][ 3.97 ]\n",
            "Validating...\n",
            "Average accuracy: 0.588\n",
            "Saved checkpoint.\n",
            "[ 16/300 ][ 21/21 ][ 0.00099 ][ 0:03:35 ][ 3.92 ]\n",
            "Validating...\n",
            "Average accuracy: 0.611\n",
            "Saved checkpoint.\n",
            "[ 20/300 ][ 21/21 ][ 0.00099 ][ 0:03:35 ][ 3.88 ]\n",
            "Validating...\n",
            "Average accuracy: 0.627\n",
            "Saved checkpoint.\n",
            "[ 24/300 ][ 21/21 ][ 0.00098 ][ 0:03:40 ][ 3.85 ]\n",
            "Validating...\n",
            "Average accuracy: 0.645\n",
            "Saved checkpoint.\n",
            "[ 28/300 ][ 21/21 ][ 0.00098 ][ 0:03:34 ][ 3.83 ]\n",
            "Validating...\n",
            "Average accuracy: 0.653\n",
            "Saved checkpoint.\n",
            "[ 32/300 ][ 21/21 ][ 0.00097 ][ 0:03:35 ][ 3.81 ]\n",
            "Validating...\n",
            "Average accuracy: 0.693\n",
            "Saved checkpoint.\n",
            "[ 36/300 ][ 21/21 ][ 0.00096 ][ 0:03:35 ][ 3.79 ]\n",
            "Validating...\n",
            "Average accuracy: 0.684\n",
            "[ 40/300 ][ 21/21 ][ 0.00096 ][ 0:03:35 ][ 3.77 ]\n",
            "Validating...\n",
            "Average accuracy: 0.702\n",
            "Saved checkpoint.\n",
            "[ 44/300 ][ 21/21 ][ 0.00095 ][ 0:03:35 ][ 3.75 ]\n",
            "Validating...\n",
            "Average accuracy: 0.720\n",
            "Saved checkpoint.\n",
            "[ 48/300 ][ 21/21 ][ 0.00094 ][ 0:03:36 ][ 3.74 ]\n",
            "Validating...\n",
            "Average accuracy: 0.715\n",
            "[ 52/300 ][ 21/21 ][ 0.00093 ][ 0:03:35 ][ 3.73 ]\n",
            "Validating...\n",
            "Average accuracy: 0.732\n",
            "Saved checkpoint.\n",
            "[ 56/300 ][ 21/21 ][ 0.00092 ][ 0:03:39 ][ 3.72 ]\n",
            "Validating...\n",
            "Average accuracy: 0.729\n",
            "[ 60/300 ][ 21/21 ][ 0.00090 ][ 0:03:42 ][ 3.70 ]\n",
            "Validating...\n",
            "Average accuracy: 0.746\n",
            "Saved checkpoint.\n",
            "[ 64/300 ][ 21/21 ][ 0.00089 ][ 0:03:41 ][ 3.69 ]\n",
            "Validating...\n",
            "Average accuracy: 0.754\n",
            "Saved checkpoint.\n",
            "[ 68/300 ][ 21/21 ][ 0.00088 ][ 0:03:38 ][ 3.68 ]\n",
            "Validating...\n",
            "Average accuracy: 0.764\n",
            "Saved checkpoint.\n",
            "[ 72/300 ][ 21/21 ][ 0.00086 ][ 0:03:35 ][ 3.67 ]\n",
            "Validating...\n",
            "Average accuracy: 0.767\n",
            "Saved checkpoint.\n",
            "[ 76/300 ][ 21/21 ][ 0.00085 ][ 0:03:35 ][ 3.65 ]\n",
            "Validating...\n",
            "Average accuracy: 0.771\n",
            "Saved checkpoint.\n",
            "[ 80/300 ][ 21/21 ][ 0.00083 ][ 0:03:40 ][ 3.65 ]\n",
            "Validating...\n",
            "Average accuracy: 0.778\n",
            "Saved checkpoint.\n",
            "[ 84/300 ][ 21/21 ][ 0.00082 ][ 0:03:36 ][ 3.64 ]\n",
            "Validating...\n",
            "Average accuracy: 0.783\n",
            "Saved checkpoint.\n",
            "[ 88/300 ][ 21/21 ][ 0.00080 ][ 0:03:36 ][ 3.63 ]\n",
            "Validating...\n",
            "Average accuracy: 0.781\n",
            "[ 92/300 ][ 21/21 ][ 0.00079 ][ 0:03:34 ][ 3.62 ]\n",
            "Validating...\n",
            "Average accuracy: 0.773\n",
            "[ 96/300 ][ 21/21 ][ 0.00077 ][ 0:03:34 ][ 3.61 ]\n",
            "Validating...\n",
            "Average accuracy: 0.794\n",
            "Saved checkpoint.\n",
            "[ 100/300 ][ 21/21 ][ 0.00075 ][ 0:03:36 ][ 3.61 ]\n",
            "Validating...\n",
            "Average accuracy: 0.787\n",
            "[ 104/300 ][ 21/21 ][ 0.00073 ][ 0:03:35 ][ 3.60 ]\n",
            "Validating...\n",
            "Average accuracy: 0.804\n",
            "Saved checkpoint.\n",
            "[ 108/300 ][ 21/21 ][ 0.00071 ][ 0:03:36 ][ 3.59 ]\n",
            "Validating...\n",
            "Average accuracy: 0.801\n",
            "[ 112/300 ][ 21/21 ][ 0.00069 ][ 0:03:35 ][ 3.58 ]\n",
            "Validating...\n",
            "Average accuracy: 0.810\n",
            "Saved checkpoint.\n",
            "[ 116/300 ][ 21/21 ][ 0.00067 ][ 0:03:37 ][ 3.58 ]\n",
            "Validating...\n",
            "Average accuracy: 0.807\n",
            "[ 120/300 ][ 21/21 ][ 0.00065 ][ 0:03:34 ][ 3.58 ]\n",
            "Validating...\n",
            "Average accuracy: 0.810\n",
            "[ 124/300 ][ 21/21 ][ 0.00063 ][ 0:03:35 ][ 3.57 ]\n",
            "Validating...\n",
            "Average accuracy: 0.807\n",
            "[ 128/300 ][ 21/21 ][ 0.00061 ][ 0:03:35 ][ 3.56 ]\n",
            "Validating...\n",
            "Average accuracy: 0.811\n",
            "Saved checkpoint.\n",
            "[ 132/300 ][ 21/21 ][ 0.00059 ][ 0:03:35 ][ 3.56 ]\n",
            "Validating...\n",
            "Average accuracy: 0.815\n",
            "Saved checkpoint.\n",
            "[ 136/300 ][ 21/21 ][ 0.00057 ][ 0:03:33 ][ 3.55 ]\n",
            "Validating...\n",
            "Average accuracy: 0.821\n",
            "Saved checkpoint.\n",
            "[ 140/300 ][ 21/21 ][ 0.00055 ][ 0:03:35 ][ 3.55 ]\n",
            "Validating...\n",
            "Average accuracy: 0.816\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-032a227a2b2f>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mstep_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_EPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-3e522132816d>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRandomly\u001b[0m \u001b[0mflipped\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \"\"\"\n\u001b[0;32m--> 712\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoSoCNwwLnlz",
        "outputId": "ae184fff-4da7-4e4a-f2b5-08060e8a50e4"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===========================================================================\n",
              "Layer (type:depth-idx)                             Param #\n",
              "===========================================================================\n",
              "DataParallel                                       --\n",
              "├─ViT: 1-1                                         25,344\n",
              "│    └─PatchEmbedding: 2-1                         --\n",
              "│    │    └─LayerNorm: 3-1                         96\n",
              "│    │    └─Linear: 3-2                            18,816\n",
              "│    │    └─Dropout: 3-3                           --\n",
              "│    │    └─LayerNorm: 3-4                         768\n",
              "│    └─Dropout: 2-2                                --\n",
              "│    └─TransformerEncoder: 2-3                     --\n",
              "│    │    └─ModuleList: 3-5                        5,322,240\n",
              "│    └─LayerNorm: 2-4                              768\n",
              "│    └─Linear: 2-5                                 3,850\n",
              "│    └─Dropout: 2-6                                --\n",
              "===========================================================================\n",
              "Total params: 5,371,882\n",
              "Trainable params: 5,371,882\n",
              "Non-trainable params: 0\n",
              "==========================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gjb6oJNpLsG6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}